{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mEtiO64_CHEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import collections\n",
        "import shutil\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "def mkdir_if_not_exist(path_list : List[str]):\n",
        "    \"\"\"Make a directory if it does not exist.\"\"\"\n",
        "    path = os.path.join(*path_list)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label):\n",
        "  min_n_train_per_label = (\n",
        "      collections.Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
        "\n",
        "  n_valid_per_label = math.floor(min_n_train_per_label * valid_ratio)\n",
        "  label_count = {}\n",
        "  for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
        "    idx = train_file.split('.')[0]\n",
        "    label = idx_label[idx]\n",
        "\n",
        "    mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
        "\n",
        "    shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
        "                os.path.join(data_dir, input_dir, 'train_valid', label))\n",
        "\n",
        "    if label not in label_count or label_count[label] < n_valid_per_label:\n",
        "      mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
        "      shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
        "                  os.path.join(data_dir, input_dir, 'valid', label))\n",
        "      label_count[label] = label_count.get(label, 0) + 1\n",
        "\n",
        "    else:\n",
        "      mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
        "      shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
        "                  os.path.join(data_dir, input_dir, 'train', label))\n",
        "\n",
        "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n",
        "  with open(os.path.join(data_dir, label_file), 'r') as f:\n",
        "    lines = f.readlines()[1:]\n",
        "    tokens = [l.rstrip().split(',') for l in lines]\n",
        "    idx_label = dict(((idx, label) for idx, label in tokens))\n",
        "\n",
        "  reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label)\n",
        "\n",
        "  mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
        "  for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
        "    shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
        "                os.path.join(data_dir, input_dir, 'test', 'unknown'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  data_dir, label_file, train_dir, test_dir = '/content/drive/MyDrive/ICT303/dog-breed-identification', 'labels.csv', 'train', 'test'\n",
        "  input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n",
        "  reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
      ],
      "metadata": {
        "id": "BogY6iuyl8VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, dim=1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "tG-c9D_Wl8SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "mZRkTwkwCGyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "class Validator():\n",
        "\n",
        "    def __init__(self, val_folder: Path, val_interval: int = 2) -> None:\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.val_folder = val_folder\n",
        "        self.val_interval = val_interval\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.dataset = datasets.ImageFolder(self.val_folder, transform=self.transform)\n",
        "\n",
        "    def validate_model(self, model: torch.nn.Module, criterion, batch_size: int = 32):\n",
        "\n",
        "\n",
        "        assert model is not None, \"Model is not defined\"\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        model.eval()\n",
        "        total_accuracy = 0.0\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_i, (inputs, labels) in enumerate(dataloader):\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                batch_accuracy = accuracy(outputs, labels)\n",
        "                total_accuracy += batch_accuracy\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if (batch_i + 1) % 10 == 0:\n",
        "                    print(f\"Val Step [{batch_i + 1}/{len(dataloader)}], Loss: {running_loss / 10:.4f} Accuracy: {total_accuracy / (batch_i +1):.4f}\")\n",
        "                    running_loss = 0.0\n",
        "\n",
        "            val_metric = total_accuracy / (batch_i + 1)\n",
        "            print(f\"Validation accuracy: {val_metric:.4f}\")\n",
        "            print(\"last output\", self._prediction_to_csv_str(torch.nn.functional.softmax(outputs[0]).detach().cpu().numpy()))\n",
        "\n",
        "            return val_metric\n",
        "\n",
        "    def test_on_folder(self, model: torch.nn.Module, test_folder: Path, output_file: Path, batch_size: int = 64):\n",
        "\n",
        "\n",
        "        if output_file.exists():\n",
        "            print(f\"Output file {output_file} already exists. Exiting...\")\n",
        "            return\n",
        "\n",
        "        print(f\"Testing model on {test_folder}\")\n",
        "\n",
        "        csv_output = \"id,\" + \",\".join(self._get_class_names()) + \"\\n\"\n",
        "        batch_img_tensors = []\n",
        "        batch_img_paths = []\n",
        "        batch_count = 0\n",
        "        total_count = 0\n",
        "        n_test_imgs = len(list(test_folder.iterdir()))\n",
        "        for img_path in test_folder.iterdir():\n",
        "            if img_path.is_file():\n",
        "                img_data = Image.open(img_path)\n",
        "                img_tensor = self.transform(img_data)\n",
        "                img_tensor = img_tensor.unsqueeze(0)\n",
        "                batch_img_tensors.append(img_tensor)\n",
        "                batch_img_paths.append(img_path)\n",
        "\n",
        "                if batch_count == batch_size or total_count == n_test_imgs - 1:\n",
        "                    batch_count = 0\n",
        "                    img_tensor = torch.cat(batch_img_tensors)\n",
        "\n",
        "                    img_tensor = img_tensor.to(self.device)\n",
        "                    outputs = torch.nn.functional.softmax(model(img_tensor), dim=1).cpu().detach().numpy()\n",
        "\n",
        "                    for output, path in zip(outputs, batch_img_paths):\n",
        "                        csv_line = f\"{path.stem},{self._prediction_to_csv_str(output)}\\n\"\n",
        "                        csv_output += csv_line\n",
        "\n",
        "                    batch_img_tensors = []\n",
        "                    batch_img_paths = []\n",
        "\n",
        "                batch_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(csv_output)\n",
        "\n",
        "        print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "    def _get_class_names(self):\n",
        "        return self.dataset.classes\n",
        "\n",
        "    def _prediction_to_csv_str(self, pred_probs: np.array, sep=\",\") -> str:\n",
        "\n",
        "        result_str = \"\"\n",
        "        for class_prob in pred_probs:\n",
        "            if class_prob < 0:\n",
        "                class_prob *= -1\n",
        "            result_str += f\"{class_prob:.4f}{sep}\"\n",
        "        return result_str[:-1]\n"
      ],
      "metadata": {
        "id": "qUrBQTU_l8MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "\n",
        "    def __init__(self, train_folder : Path, save_folder : Path, n_epochs : int, batch_size : int, augmetation_transforms) -> None:\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.train_folder = train_folder\n",
        "        self.save_folder = save_folder\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.augmetation_transforms = augmetation_transforms\n",
        "        self._prepare_data()\n",
        "        self._prepare_model()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "\n",
        "        transform_steps = [\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)\n",
        "        ]\n",
        "\n",
        "        if self.augmetation_transforms is not None and len(self.augmetation_transforms) > 0:\n",
        "            transform_steps.extend(self.augmetation_transforms)\n",
        "\n",
        "        transform_steps.append(transforms.ToTensor())\n",
        "        transform_steps.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "\n",
        "        transform = transforms.Compose(transform_steps)\n",
        "\n",
        "\n",
        "        self.dataset = datasets.ImageFolder(self.train_folder, transform=transform)\n",
        "\n",
        "    def _prepare_model(self):\n",
        "\n",
        "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        num_classes = len(self.dataset.classes)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def load_model_from_file(self, file : Path):\n",
        "        self.model.load_state_dict(torch.load(file))\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def train(self, validator : Validator):\n",
        "\n",
        "\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        # optimizer = optim.AdamW(self.model.parameters(), lr=0.001)\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
        "       # optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "        best_val_score = 0.0\n",
        "        for epoch in range(self.n_epochs):\n",
        "            running_loss = 0.0\n",
        "            self.model.train()\n",
        "            for i, (inputs, labels) in enumerate(dataloader):\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    print(f\"Epoch [{epoch + 1}/{self.n_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {running_loss / 10:.4f}\")\n",
        "                    running_loss = 0.0\n",
        "\n",
        "            if (epoch + 1) % validator.val_interval == 0:\n",
        "                val_score = validator.validate_model(self.model, criterion, batch_size=self.batch_size)\n",
        "\n",
        "                if val_score > best_val_score:\n",
        "                    best_val_score = val_score\n",
        "                    torch.save(self.model.state_dict(), self.save_folder / \"best_model.pt\")\n",
        "                    print(\"Best model saved!\")\n",
        "\n",
        "        print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "7U7jtSCQl8JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data_augementation_steps = [\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    #transforms.RandomRotation(degrees=20),\n",
        "   # transforms.CenterCrop(15),\n",
        "\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)\n",
        "]\n",
        "\n",
        "trainer = Trainer(train_folder=Path(\"/content/drive/MyDrive/ICT303/dog-breed-identification/train_valid_test/train\"),\n",
        "                 save_folder=Path(\"/content/drive/MyDrive/ICT303/dog-breed-identification\"),\n",
        "                    n_epochs=10,\n",
        "                    batch_size=128,\n",
        "                    augmetation_transforms=data_augementation_steps\n",
        "                 )\n",
        "validator = Validator(val_folder=Path(\"/content/drive/MyDrive/ICT303/dog-breed-identification/train_valid_test/valid\"), val_interval=1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"start training ...\")\n",
        "trainer.train(validator)\n",
        "trainer.load_model_from_file(Path(\"/content/drive/MyDrive/ICT303/dog-breed-identification/best_model.pt\"))\n",
        "validator.test_on_folder(trainer.model, test_folder=Path(\"/content/drive/MyDrive/ICT303/dog-breed-identification/test\"), output_file=Path(\"test_results.csv\"))"
      ],
      "metadata": {
        "id": "Tk4jRGyaCeMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is the script for training the deep learning model on a dog bread identification dataset.It uses ResNet-50 architecture , which is a popular conventional neural network architecture(CNN), which is known for its good performance on various image classification tasks."
      ],
      "metadata": {
        "id": "KNVTRX-n1fPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Break down of the code:\n",
        "\n",
        "First the functions reorg_train_valid and reorg_dog_data for reorganizing the data into train, validation, and test sets are defined.The function reorg_dog_data takes the data directory, label file, train, and test directories, and a validation ratio as input. It separates the data into train, validation, and test folders based on the provided ratio.\n",
        "\n",
        "Then The Validator class is defined to handle the validation process. It takes the validation folder (after reorganizing the data), and validation interval as input. It uses PyTorch's ImageFolder dataset to load the images and apply required transformations for validation.\n",
        "\n",
        "The test_on_folder method in the Validator class is used for testing the model on the test dataset. It loads the test images, performs inference, and saves the predictions to a CSV file.\n",
        "\n",
        "The Trainer class is defined to handle the training process. It takes the train folder, save folder, number of epochs, batch size, and optional data augmentation transformations as input. It uses the ResNet-50 architecture and adapts the last fully connected layer to match the number of classes in the dataset.\n",
        "The training loop iterates through the dataset for the specified number of epochs, using cross-entropy loss and stochastic gradient descent (SGD) as the optimizer.\n",
        "\n",
        "There is  a list of data augmentation transformations defined, such as random horizontal and vertical flips, color jitter, etc. These augmentations help in creating variations of the input data during training, which can lead to better generalization and performance.\n",
        "I have chosen the ones that give better result than others after trying them out.\n",
        "\n",
        "The script creates an instance of the Trainer class and an instance of the Validator class. It then performs the training using the trainer.train(validator) method, which iterates through the dataset and trains the ResNet-50 model.\n",
        "After training, the best model is loaded back from the saved file (best_model.pt) using trainer.load_model_from_file method.\n",
        "The validator.test_on_folder method is used to test the trained model on the test dataset, and the results are saved to test_results.csv.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DHMvfD981fGc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}